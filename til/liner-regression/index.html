<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Dinis Peixoto]">
<meta name="description" content="Fundamentals Linear regression is a technique used to find the relationship between variables, in the context of machine learning that essentially means the relationship between features and labels.
It can be represented as follows:
$$ \hat{y} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\hat{y}\) is the label \(\text{b}\) is the bias \(w_i\) is the weight of the feature i \(x_i\) is the feature value of feature i Loss Loss is usually the metric we optimize the model for, it essentially tells us how wrong the model is - it measures the distance between the model&amp;rsquo;s predictions (\(\text{y}\)) and the actual labels (\(\hat{y}\))." />
<meta name="keywords" content="dinis peixoto, machine learning, machine learning engineering, mlops, computer science, today I learned, blog, hugo, software engineering, resume, machine learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://dinispeixoto.com/til/liner-regression/" />


    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>



    <title>
        
            Liner Regression from scratch :: Dinis Peixoto 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://dinispeixoto.com/main.d2b9fd41d7e7e3c5253cd69761a2dfc0c0ffbff5d78e06960781a5ca3e8bf4ea.css">



    <link rel="apple-touch-icon" sizes="180x180" href="https://dinispeixoto.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dinispeixoto.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dinispeixoto.com/favicon-16x16.png">
    <link rel="manifest" href="https://dinispeixoto.com/site.webmanifest">
    <link rel="mask-icon" href="https://dinispeixoto.com/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="https://dinispeixoto.com/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Liner Regression from scratch">
<meta itemprop="description" content="Fundamentals Linear regression is a technique used to find the relationship between variables, in the context of machine learning that essentially means the relationship between features and labels.
It can be represented as follows:
$$ \hat{y} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\hat{y}\) is the label \(\text{b}\) is the bias \(w_i\) is the weight of the feature i \(x_i\) is the feature value of feature i Loss Loss is usually the metric we optimize the model for, it essentially tells us how wrong the model is - it measures the distance between the model&rsquo;s predictions (\(\text{y}\)) and the actual labels (\(\hat{y}\))."><meta itemprop="datePublished" content="2025-06-18T00:00:00+00:00" />
<meta itemprop="dateModified" content="2025-06-18T00:00:00+00:00" />
<meta itemprop="wordCount" content="969"><meta itemprop="image" content="https://dinispeixoto.com/img/dinis1.jpg" />
<meta itemprop="keywords" content="machine learning," />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://dinispeixoto.com/img/dinis1.jpg" /><meta name="twitter:title" content="Liner Regression from scratch"/>
<meta name="twitter:description" content="Fundamentals Linear regression is a technique used to find the relationship between variables, in the context of machine learning that essentially means the relationship between features and labels.
It can be represented as follows:
$$ \hat{y} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\hat{y}\) is the label \(\text{b}\) is the bias \(w_i\) is the weight of the feature i \(x_i\) is the feature value of feature i Loss Loss is usually the metric we optimize the model for, it essentially tells us how wrong the model is - it measures the distance between the model&rsquo;s predictions (\(\text{y}\)) and the actual labels (\(\hat{y}\))."/>



    <meta property="og:title" content="Liner Regression from scratch" />
<meta property="og:description" content="Fundamentals Linear regression is a technique used to find the relationship between variables, in the context of machine learning that essentially means the relationship between features and labels.
It can be represented as follows:
$$ \hat{y} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\hat{y}\) is the label \(\text{b}\) is the bias \(w_i\) is the weight of the feature i \(x_i\) is the feature value of feature i Loss Loss is usually the metric we optimize the model for, it essentially tells us how wrong the model is - it measures the distance between the model&rsquo;s predictions (\(\text{y}\)) and the actual labels (\(\hat{y}\))." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dinispeixoto.com/til/liner-regression/" /><meta property="og:image" content="https://dinispeixoto.com/img/dinis1.jpg" /><meta property="article:section" content="til" />
<meta property="article:published_time" content="2025-06-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-06-18T00:00:00+00:00" />
<meta property="og:see_also" content="https://dinispeixoto.com/til/redis-performance/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/calibrated-probabilities/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/distributed-model-inference-spark/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/kafka-exactly-once-semantics/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/spark-architecture/" />






    <meta property="article:section" content="Machine Learning Engineering" />



    <meta property="article:published_time" content="2025-06-18 00:00:00 &#43;0000 UTC" />









    
<script>
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-170600213-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://dinispeixoto.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text"> cd $HOME</span>
            <span class="logo__cursor" style=
                  "
                   
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dinispeixoto.com/about/">about me</a></li><li><a href="https://dinispeixoto.com/posts/">posts</a></li><li><a href="https://dinispeixoto.com/til/">today I learned</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dinispeixoto.com/til/liner-regression/">Liner Regression from scratch</a></h2>

            
            
            

            <div class="post-content">
                <h3 id="fundamentals">Fundamentals</h3>
<p>Linear regression is a technique used to find the relationship between variables, in the context of machine learning that essentially means the relationship between features and labels.</p>
<p>It can be represented as follows:</p>
$$
\hat{y} = \text{b} + w_1 x_1 + w_2 x_2 + \dots + w_n x_n
$$
<ul>
<li>\(\hat{y}\) is the label</li>
<li>\(\text{b}\) is the bias</li>
<li>\(w_i\) is the weight of the feature i</li>
<li>\(x_i\) is the feature value of feature i</li>
</ul>
<h3 id="loss">Loss</h3>
<p>Loss is usually the metric we optimize the model for, it essentially tells us how wrong the model is - it measures the distance between the model&rsquo;s predictions (\(\text{y}\)) and the actual labels (\(\hat{y}\)). Something worth keeping in mind that is we don&rsquo;t really care about the direction, only the distance between the values, so loss functions usually removing the sign - i.e. 2 - 5 = -3, loss is 3.</p>
<table>
<thead>
<tr>
<th><strong>Loss Type</strong></th>
<th><strong>Definition</strong></th>
<th><strong>Equation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>L1 Loss</strong></td>
<td>Sum of the absolute differences between predicted and actual values.</td>
<td>$$L = \sum_{i=1}^{N} \lvert y_i - \hat{y}_i \rvert$$</td>
</tr>
<tr>
<td><strong>L2 Loss</strong></td>
<td>Sum of the squared differences between predicted and actual values.</td>
<td>$$L = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$</td>
</tr>
<tr>
<td><strong>Mean Absolute Error (MAE)</strong></td>
<td>Average of L1 loss across <em>N</em> examples.</td>
<td>$$MAE = \frac{1}{N} \sum_{i=1}^{N} \lvert y_i - \hat{y}_i \rvert$$</td>
</tr>
<tr>
<td><strong>Mean Squared Error (MSE)</strong></td>
<td>Average of L2 loss across <em>N</em> examples.</td>
<td>$$MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$</td>
</tr>
</tbody>
</table>
<p>MSE is useful for outliers, it moves the model more towards it as it reduces the loss, while MAE might keep the model closer to predictions that are not necessarily an outlier.</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>The goal of training model is essentially figuring out the values for \(\text{b}\) and \(w_i\) for every feature i that minimizes the loss function picked. This is where gradient descent comes in, it iteratively finds the weights and bias that produce the model with the lowest loss. It does so by doing the following process:</p>
<ol>
<li>Calculate loss with current weights and bias</li>
<li>For each weight and bias, determine the direction in which we should move them to reduce loss (opposite of the gradient)</li>
<li>For each weight and bias, update them by moving a small amount in the direction found</li>
<li>Return to step 1 and repeat until we the model can&rsquo;t reduce the loss any further</li>
</ol>
<h4 id="1-start-with-weight-and-bias-as-0">1. Start with weight and bias as 0</h4>
<p>We can assume that weights and bias can start as 0 when training starts.</p>
$$
\hat{y} = 0 + 0 \cdot x
$$
<h4 id="2-calculate-mse-loss-with-the-current-model-parameters">2. Calculate MSE loss with the current model parameters</h4>
<p>When calculating MSE for the model above, we would end up with something like the following if we assume expected values of 18, 15 and so on for our model.</p>
$$ Loss = ((18 - 0)^2 + (15 - 0)^2 ) + ... / N $$
<p>This is obviously not going to result in the best model predictions, so our goal now becomes finding which bias and weights take us to the best possible model, thus reducing the loss.</p>
<h4 id="3-calculate-the-slope-of-the-tangent-to-the-loss-function-for-each-weight-and-bias-ie-the-gradient">3. Calculate the slope of the tangent to the loss function for each weight and bias (i.e. the gradient)</h4>
<p>We can only optimize the loss if we understand exacly how to update each value, in this case weight and bias, in a way that results in a lower loss. This is where gradients come in.</p>
<p>The gradient is essentially how much y changes when x increases by 1, it tells us in which direction the function is taking us - e.g. m is the gradient in \(y = mx + b\).</p>
<p>To get the slope for the lines tangent to the weight and bias, we take the derivative of the loss function with respect to the weight and the bias, and then solve the equations.</p>
<p>Again, assuming we use MSE as our loss function \( \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 \) and \(y_i = w \cdot x + b\), as it represents our model.</p>
<p><strong>Weight derivative</strong></p>
<p>The weight derivative is \( \frac{\partial}{\partial w} ( (w \cdot x + b - y)^2 ) \), we can apply the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>.</p>
<p>We assume \(u^2\), where \(u = (w \cdot x + b - y)\)</p>
<p>So \( \frac{d}{dw} (u^2) = 2u \cdot \frac{du}{dw} \)</p>
<p>Since \(\frac{du}{dw} = x\)</p>
<p>It becomes \( \frac{d}{dw} ( (w \cdot x + b - y)^2 ) = 2(w \cdot x + b - y) \cdot x \).</p>
<p>We need to do this for every sample in the training dataset, so it&rsquo;s actually:</p>
$$ \frac{\partial \text{MSE}}{\partial w} = \frac{1}{M} \sum_{i=1}^{M} 2(w \cdot x^{(i)} + b - y^{(i)}) \cdot x^{(i)} $$
<p><strong>Bias derivative</strong></p>
<p>If we do the same for the bias, we would instead get:</p>
$$ \frac{\partial \text{MSE}}{\partial b} = \frac{1}{M} \sum_{i=1}^{M} 2(w \cdot x^{(i)} + b - y^{(i)}) $$
<h4 id="4-update-the-weights">4. Update the weights</h4>
<p>Well, now that we know exactly how to calculate the gradients, this should enable us to update both weight and bias in a way that optimizes loss.</p>
<p>The gradient tells us the opposite direction of where we have to go. An easy way to understand this is essentially thinking that if the gradient for a loss function is 5, every time we increase the weight by 1 unit, the loss will increase by 5, so essentially we want to do the opposite as our goal is to decrease the loss.</p>
$$ \text{new weight} = \text{old weight} - \eta \cdot \text{weight's gradient} $$
$$ \text{new bias} = \text{old bias} - \eta \cdot \text{bias' gradient} $$
<p>The small value used to update the gradient is the <em>learning rate</em>, which we will be leaving out of scope for now.</p>
<h4 id="5-model-training">5. Model Training</h4>
<p>After iteratively applying the steps above, model training would look something like this:</p>
<table>
<thead>
<tr>
<th>Iteration</th>
<th>Weight</th>
<th>Bias</th>
<th>Loss (MSE)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>303.71</td>
</tr>
<tr>
<td>2</td>
<td>1.20</td>
<td>0.34</td>
<td>170.84</td>
</tr>
<tr>
<td>3</td>
<td>2.05</td>
<td>0.59</td>
<td>103.17</td>
</tr>
<tr>
<td>4</td>
<td>2.66</td>
<td>0.78</td>
<td>68.70</td>
</tr>
<tr>
<td>5</td>
<td>3.09</td>
<td>0.91</td>
<td>51.13</td>
</tr>
<tr>
<td>6</td>
<td>3.40</td>
<td>1.01</td>
<td>42.17</td>
</tr>
</tbody>
</table>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://dinispeixoto.com/tags/machine-learning/">machine learning</a></span>
        
    </p>

            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="https://dinispeixoto.com/categories/machine-learning-engineering/">Machine Learning Engineering</a></span>
        
    </p>

  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span></span>
            <span>&copy; 2025</span>
            
                <span><a href="https://dinispeixoto.com/">Dinis Peixoto</a></span>
            
            <span>Powered by <a href="http://gohugo.io">Hugo</a> & <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng">Hello Friend NG</a></span>
            <span></span>
        </div>
    </div>
</footer>



            
        </div>

        



<script type="text/javascript" src="https://dinispeixoto.com/bundle.min.b1dc444a7108174ae3d2294f982be5e99e844ae2b8d58eeea19d929bde0338be4d12ed8a4dfc67194dcc6726fa6a82705119c0d7e4edd4de9294be6535ea7b7f.js" integrity="sha512-sdxESnEIF0rj0ilPmCvl6Z6ESuK41Y7uoZ2Sm94DOL5NEu2KTfxnGU3MZyb6aoJwURnA1&#43;Tt1N6SlL5lNep7fw=="></script>



    </body>
</html>
