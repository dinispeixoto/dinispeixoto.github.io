<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Dinis Peixoto]">
<meta name="description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
 text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model  I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&amp;rsquo;s explained here)." />
<meta name="keywords" content="dinis peixoto, computer science, today I learned, blog, hugo, software engineering, resume, spark, batch, bert, vit, pytorch, machine learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://dinispeixoto.com/til/distributed-model-inference-spark/" />


    <title>
        
            Distributed model inference using Spark :: Dinis Peixoto 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://dinispeixoto.com/main.ae882888efdfe270720245beed0f50838debe778b75c1df507f4ecf20e9ea7ec.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://dinispeixoto.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dinispeixoto.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dinispeixoto.com/favicon-16x16.png">
    <link rel="manifest" href="https://dinispeixoto.com/site.webmanifest">
    <link rel="mask-icon" href="https://dinispeixoto.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://dinispeixoto.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">



<meta itemprop="name" content="Distributed model inference using Spark">
<meta itemprop="description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
 text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model  I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained here).">
<meta itemprop="datePublished" content="2022-04-06T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2022-04-06T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="445">
<meta itemprop="image" content="https://dinispeixoto.com/img/image_original_3.jpg"/>



<meta itemprop="keywords" content="spark,batch,bert,vit,pytorch,machine learning," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://dinispeixoto.com/img/image_original_3.jpg"/>

<meta name="twitter:title" content="Distributed model inference using Spark"/>
<meta name="twitter:description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
 text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model  I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained here)."/>



    <meta property="og:title" content="Distributed model inference using Spark" />
<meta property="og:description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
 text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model  I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained here)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dinispeixoto.com/til/distributed-model-inference-spark/" />
<meta property="og:image" content="https://dinispeixoto.com/img/image_original_3.jpg"/>
<meta property="article:published_time" content="2022-04-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-04-06T00:00:00+00:00" />





    <meta property="article:section" content="Machine Learning Engineering" />



    <meta property="article:published_time" content="2022-04-06 00:00:00 &#43;0000 UTC" />








    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://dinispeixoto.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text"> cd $HOME</span>
            <span class="logo__cursor" style=
                  "
                   
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dinispeixoto.com/about/">about me</a></li><li><a href="https://dinispeixoto.com/posts/">posts</a></li><li><a href="https://dinispeixoto.com/til/">today I learned</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dinispeixoto.com/til/distributed-model-inference-spark/">Distributed model inference using Spark</a></h2>

            
            
            

            <div class="post-content">
                <p>During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:</p>
<ul>
<li>text embeddings using a fine-tuned <em>BERT</em> model</li>
<li>image embeddings using a fine-tuned <em>ViT</em> model</li>
</ul>
<p>I will use this TIL to share some of the key learnings I could get from this experience.</p>
<h3 id="pandas-udf-vs-spark-udf">Pandas UDF vs Spark UDF</h3>
<p>Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained <a href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html">here</a>).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a6e22e">@pandas_udf</span>(<span style="color:#f92672">...</span>)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(batch: pd<span style="color:#f92672">.</span>Series) <span style="color:#f92672">-&gt;</span> pd<span style="color:#f92672">.</span>Series:
  <span style="color:#f92672">...</span>
</code></pre></div><h3 id="broadcasting-the-model">Broadcasting the model</h3>
<p>Broadcasting the model to the workers should help as a read-only copy is kept on each spark worker (it&rsquo;s also possible to broadcast the <code>state_dict</code> only - example for PyTorch - and load the model every time your Pandas UDF is called, just make sure you are using a large batch otherwise you will spend more time loading the model than actually running inference).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">broadcasted_model <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>broadcast(model)
</code></pre></div><h3 id="gpu-vs-cpu">GPU vs CPU</h3>
<p>Inference using a single GPU was much faster than using a CPU-based cluster (even with 8 cores/worker, which is basically having 8 threads/worker running in parallel).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>to(device)
</code></pre></div><h3 id="predictions-in-batch">Predictions in batch</h3>
<p>Run predictions in batch e.g. using PyTorch&rsquo;s <code>DataLoader</code> helps a lot. This means that you are going to run inference for multiple inputs at a time (you can update the batch size according to both the GPU usage and memory available), also don&rsquo;t mistake this with the Pandas UDF batches, they are two different things.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader

dataloader <span style="color:#f92672">=</span> DataLoader(input_dataset, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>)

<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> dataloder:
  batch<span style="color:#f92672">.</span>to(device)
  model<span style="color:#f92672">.</span>predict(batch)
</code></pre></div><h3 id="gpu-usage">GPU usage</h3>
<p>Always validate the GPU usage. Try to keep it above 90% to make sure the time is actually being spent with inference and not with the iterator or something else - this can easily be checked through&rsquo;s Ganglia UI if you are using Databricks.</p>
<h3 id="hdfs---small-files-problem">HDFS - Small files problem</h3>
<p>When using images from an HDFS, consider loading them first to a table on Databricks - reading each image from the filesystem is really expensive when each image is a separate file. I opted for loading them to a delta table to have metadata along with less partitioned files, which also considerably improved the loading performance.</p>
<h3 id="always-validate-the-number-of-partitions">Always validate the number of partitions</h3>
<p>This last one might sound a bit stupid but always make sure you are using multiple spark workers as well - while debugging I ran multiple experiments with <code>df.limit(x)</code>, the thing is that this returns a dataframe with a single partition, thus only one spark worker was actually running the inference.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bert">Transformers - BERT</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/vit">Transformers - ViT</a></li>
<li><a href="https://www.linkedin.com/pulse/apache-spark-small-file-problem-simple-advanced-solutions-garg/">Spark - Small files problem</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">PyTorch&rsquo;s DataLoader</a></li>
</ul>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://dinispeixoto.com/tags/spark/">spark</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/batch/">batch</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/bert/">bert</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/vit/">vit</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/pytorch/">pytorch</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/machine-learning/">machine learning</a></span>
        
    </p>

            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="https://dinispeixoto.com/categories/machine-learning-engineering/">Machine Learning Engineering</a></span>
        
    </p>

  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span></span>
            <span>&copy; 2022</span>
            
                <span><a href="https://dinispeixoto.com">Dinis Peixoto</a></span>
            
            <span>Powered by <a href="http://gohugo.io">Hugo</a> & <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng">Hello Friend NG</a></span>
            <span></span>
        </div>
    </div>
</footer>



            
        </div>

        



<script type="text/javascript" src="https://dinispeixoto.com/bundle.min.af435e44374f1e99a669ea8cd5bb9a2fceed80588941a451bfddb66b86a67c9f40b0f417e9543a763f809aa7e9300d7b1d69bf99615810ba02ac70396d50fad5.js" integrity="sha512-r0NeRDdPHpmmaeqM1buaL87tgFiJQaRRv922a4amfJ9AsPQX6VQ6dj&#43;AmqfpMA17HWm/mWFYELoCrHA5bVD61Q=="></script>
    
<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-170600213-1', 'auto');
	
	ga('send', 'pageview');
}
</script>




    </body>
</html>
