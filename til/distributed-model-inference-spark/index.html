<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Dinis Peixoto]">
<meta name="description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&amp;rsquo;s explained here)." />
<meta name="keywords" content="dinis peixoto, machine learning, machine learning engineering, mlops, computer science, today I learned, blog, hugo, software engineering, resume, spark, batch, bert, vit, pytorch, machine learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://dinispeixoto.com/til/distributed-model-inference-spark/" />


    <title>
        
            Distributed model inference using Spark :: Dinis Peixoto 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://dinispeixoto.com/main.91a1b8502e85c50456b7be8d8b4f555fa18315bb3a1dfc78c1f2c82ba0664491.css">



    <link rel="apple-touch-icon" sizes="180x180" href="https://dinispeixoto.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dinispeixoto.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dinispeixoto.com/favicon-16x16.png">
    <link rel="manifest" href="https://dinispeixoto.com/site.webmanifest">
    <link rel="mask-icon" href="https://dinispeixoto.com/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="https://dinispeixoto.com/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Distributed model inference using Spark">
<meta itemprop="description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained here)."><meta itemprop="datePublished" content="2022-04-06T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-04-06T00:00:00+00:00" />
<meta itemprop="wordCount" content="445"><meta itemprop="image" content="https://dinispeixoto.com/img/image_original_3.jpg"/>
<meta itemprop="keywords" content="spark,batch,bert,vit,pytorch,machine learning," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://dinispeixoto.com/img/image_original_3.jpg"/>

<meta name="twitter:title" content="Distributed model inference using Spark"/>
<meta name="twitter:description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained here)."/>



    <meta property="og:title" content="Distributed model inference using Spark" />
<meta property="og:description" content="During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:
text embeddings using a fine-tuned BERT model image embeddings using a fine-tuned ViT model I will use this TIL to share some of the key learnings I could get from this experience.
Pandas UDF vs Spark UDF Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained here)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dinispeixoto.com/til/distributed-model-inference-spark/" /><meta property="og:image" content="https://dinispeixoto.com/img/image_original_3.jpg"/><meta property="article:section" content="TIL" />
<meta property="article:published_time" content="2022-04-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-04-06T00:00:00+00:00" />
<meta property="og:see_also" content="https://dinispeixoto.com/til/kafka-exactly-once-semantics/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/spark-architecture/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/column-row-databases/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/slots/" /><meta property="og:see_also" content="https://dinispeixoto.com/til/new-vs-init/" />





    <meta property="article:section" content="Machine Learning Engineering" />



    <meta property="article:published_time" content="2022-04-06 00:00:00 &#43;0000 UTC" />








    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://dinispeixoto.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text"> cd $HOME</span>
            <span class="logo__cursor" style=
                  "
                   
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dinispeixoto.com/about/">about me</a></li><li><a href="https://dinispeixoto.com/posts/">posts</a></li><li><a href="https://dinispeixoto.com/til/">today I learned</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dinispeixoto.com/til/distributed-model-inference-spark/">Distributed model inference using Spark</a></h2>

            
            
            

            <div class="post-content">
                <p>During the past few days, I have been working on running batch inference for a large amount of data using Spark. The goal was to generate embeddings:</p>
<ul>
<li>text embeddings using a fine-tuned <em>BERT</em> model</li>
<li>image embeddings using a fine-tuned <em>ViT</em> model</li>
</ul>
<p>I will use this TIL to share some of the key learnings I could get from this experience.</p>
<h3 id="pandas-udf-vs-spark-udf">Pandas UDF vs Spark UDF</h3>
<p>Pandas UDF instead of Spark UDF helps improve performance, also allows running in batches of rows/partition instead of a single row/partition (everything&rsquo;s explained <a href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html">here</a>).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@pandas_udf</span>(<span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(batch: pd<span style="color:#f92672">.</span>Series) <span style="color:#f92672">-&gt;</span> pd<span style="color:#f92672">.</span>Series:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">...</span>
</span></span></code></pre></div><h3 id="broadcasting-the-model">Broadcasting the model</h3>
<p>Broadcasting the model to the workers should help as a read-only copy is kept on each spark worker (it&rsquo;s also possible to broadcast the <code>state_dict</code> only - example for PyTorch - and load the model every time your Pandas UDF is called, just make sure you are using a large batch otherwise you will spend more time loading the model than actually running inference).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>broadcasted_model <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>broadcast(model)
</span></span></code></pre></div><h3 id="gpu-vs-cpu">GPU vs CPU</h3>
<p>Inference using a single GPU was much faster than using a CPU-based cluster (even with 8 cores/worker, which is basically having 8 threads/worker running in parallel).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span></code></pre></div><h3 id="predictions-in-batch">Predictions in batch</h3>
<p>Run predictions in batch e.g. using PyTorch&rsquo;s <code>DataLoader</code> helps a lot. This means that you are going to run inference for multiple inputs at a time (you can update the batch size according to both the GPU usage and memory available), also don&rsquo;t mistake this with the Pandas UDF batches, they are two different things.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataloader <span style="color:#f92672">=</span> DataLoader(input_dataset, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> dataloder:
</span></span><span style="display:flex;"><span>  batch<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>  model<span style="color:#f92672">.</span>predict(batch)
</span></span></code></pre></div><h3 id="gpu-usage">GPU usage</h3>
<p>Always validate the GPU usage. Try to keep it above 90% to make sure the time is actually being spent with inference and not with the iterator or something else - this can easily be checked through Ganglia&rsquo;s UI if you are using Databricks.</p>
<h3 id="hdfs---small-files-problem">HDFS - Small files problem</h3>
<p>When using images from an HDFS, consider loading them first to a table on Databricks - reading each image from the filesystem is really expensive when each image is a separate file. I opted for loading them to a delta table to have metadata along with less partitioned files, which also considerably improved the loading performance.</p>
<h3 id="always-validate-the-number-of-partitions">Always validate the number of partitions</h3>
<p>This last one might sound a bit stupid but always make sure you are using multiple spark workers as well - while debugging I ran multiple experiments with <code>df.limit(x)</code>, the thing is that this returns a dataframe with a single partition, thus only one spark worker was actually running the inference.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bert">Transformers - BERT</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/vit">Transformers - ViT</a></li>
<li><a href="https://www.linkedin.com/pulse/apache-spark-small-file-problem-simple-advanced-solutions-garg/">Spark - Small files problem</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">PyTorch&rsquo;s DataLoader</a></li>
</ul>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://dinispeixoto.com/tags/spark/">spark</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/batch/">batch</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/bert/">bert</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/vit/">vit</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/pytorch/">pytorch</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/machine-learning/">machine learning</a></span>
        
    </p>

            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="https://dinispeixoto.com/categories/machine-learning-engineering/">Machine Learning Engineering</a></span>
        
    </p>

  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span></span>
            <span>&copy; 2022</span>
            
                <span><a href="https://dinispeixoto.com">Dinis Peixoto</a></span>
            
            <span>Powered by <a href="http://gohugo.io">Hugo</a> & <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng">Hello Friend NG</a></span>
            <span></span>
        </div>
    </div>
</footer>



            
        </div>

        



<script type="text/javascript" src="https://dinispeixoto.com/bundle.min.cf7fb2a9e24608a783e05e4472da2fdf008293289de1ad88d1cba5c143e7e414496dd33b6a228e92f8461ce0c8cbad3c9f9847e73e46dfbce0463393ddd1733a.js" integrity="sha512-z3&#43;yqeJGCKeD4F5Ectov3wCCkyid4a2I0culwUPn5BRJbdM7aiKOkvhGHODIy608n5hH5z5G37zgRjOT3dFzOg=="></script>



    </body>
</html>
