<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Dinis Peixoto]">
<meta name="description" content="Fundamentals Logistic Regression models, unlike Linear Regression, should help us predict the probability of a given outcome - e.g. if a payment is fraudulent or not. A Logistic Regression model, just like in Linear Regression can be represented as follows:
$$ \text{z} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\text{z}\) is the output of the linear equation, also called the log odds. \(\text{b}\) is the bias." />
<meta name="keywords" content="dinis peixoto, machine learning, machine learning engineering, mlops, computer science, today I learned, blog, hugo, software engineering, resume, machine learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://dinispeixoto.com/posts/logistic-regression-from-scratch/" />


    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>



    <title>
        
            Logistic Regression from scratch :: Dinis Peixoto 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://dinispeixoto.com/main.d2b9fd41d7e7e3c5253cd69761a2dfc0c0ffbff5d78e06960781a5ca3e8bf4ea.css">



    <link rel="apple-touch-icon" sizes="180x180" href="https://dinispeixoto.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dinispeixoto.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dinispeixoto.com/favicon-16x16.png">
    <link rel="manifest" href="https://dinispeixoto.com/site.webmanifest">
    <link rel="mask-icon" href="https://dinispeixoto.com/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="https://dinispeixoto.com/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Logistic Regression from scratch">
<meta itemprop="description" content="Fundamentals Logistic Regression models, unlike Linear Regression, should help us predict the probability of a given outcome - e.g. if a payment is fraudulent or not. A Logistic Regression model, just like in Linear Regression can be represented as follows:
$$ \text{z} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\text{z}\) is the output of the linear equation, also called the log odds. \(\text{b}\) is the bias."><meta itemprop="datePublished" content="2023-06-18T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-06-18T00:00:00+00:00" />
<meta itemprop="wordCount" content="906"><meta itemprop="image" content="https://dinispeixoto.com/img/dinis1.jpg" />
<meta itemprop="keywords" content="machine learning," />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://dinispeixoto.com/img/dinis1.jpg" /><meta name="twitter:title" content="Logistic Regression from scratch"/>
<meta name="twitter:description" content="Fundamentals Logistic Regression models, unlike Linear Regression, should help us predict the probability of a given outcome - e.g. if a payment is fraudulent or not. A Logistic Regression model, just like in Linear Regression can be represented as follows:
$$ \text{z} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\text{z}\) is the output of the linear equation, also called the log odds. \(\text{b}\) is the bias."/>



    <meta property="og:title" content="Logistic Regression from scratch" />
<meta property="og:description" content="Fundamentals Logistic Regression models, unlike Linear Regression, should help us predict the probability of a given outcome - e.g. if a payment is fraudulent or not. A Logistic Regression model, just like in Linear Regression can be represented as follows:
$$ \text{z} = \text{b} &#43; w_1 x_1 &#43; w_2 x_2 &#43; \dots &#43; w_n x_n $$ \(\text{z}\) is the output of the linear equation, also called the log odds. \(\text{b}\) is the bias." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dinispeixoto.com/posts/logistic-regression-from-scratch/" /><meta property="og:image" content="https://dinispeixoto.com/img/dinis1.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-06-18T00:00:00+00:00" />







    <meta property="article:section" content="Machine Learning Engineering" />



    <meta property="article:published_time" content="2023-06-18 00:00:00 &#43;0000 UTC" />









    
<script>
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-170600213-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://dinispeixoto.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text"> cd $HOME</span>
            <span class="logo__cursor" style=
                  "
                   
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dinispeixoto.com/about/">about me</a></li><li><a href="https://dinispeixoto.com/posts/">posts</a></li><li><a href="https://dinispeixoto.com/til/">today I learned</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        5 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://dinispeixoto.com/posts/logistic-regression-from-scratch/">Logistic Regression from scratch</a>
      </h1>

      

      

      

      <div class="post-content">
        <h3 id="fundamentals">Fundamentals</h3>
<p>Logistic Regression models, unlike Linear Regression, should help us predict the probability of a given outcome - e.g. if a payment is fraudulent or not. A Logistic Regression model, just like in Linear Regression can be represented as follows:</p>
$$
\text{z} = \text{b} + w_1 x_1 + w_2 x_2 + \dots + w_n x_n
$$
<ul>
<li>\(\text{z}\) is the output of the linear equation, also called the log odds.</li>
<li>\(\text{b}\) is the bias.</li>
<li>The \(w_i\) values are the model&rsquo;s learned weights.</li>
<li>The \(x_i\) values are the feature values for a particular example.</li>
</ul>
<p>However we need to keep in mind that the output needs to be represented as a probability instead, i.e. a value between 0 and 1. This is where log-odds come in.</p>
<p>Let&rsquo;s start by understanding what odds are.</p>
<h4 id="odds">Odds</h4>
<p>The odds of an event are calculated as the probability of the event occurring divided by the probability of the event not occurring \((p / (1-p))\).</p>
<p>Odds are not probabilities, but the ratio of something happening vs the ratio of something not happening. Probabilities are the ratio of something happening to everything that could happen.</p>
<p>Example could be odds of a given team winning a game are 5:3, so 5/3 = 1.7. The probability of the team winning the same is 5/(5+3) = 5/8 = 0.625.</p>
<h4 id="log-odds">Log-odds</h4>
<p>The odds of a team losing will always be a number from 0 to 1, e.g. 1/3 or 1/20 or 1/100. While the odds of a team winning will always be a number from 1 to \((+\infty\)) - this creates asymmetry. This is where log(odds) comes into play, as by doing the log of the odds everything becomes symmetrical instead!</p>
<p>Let&rsquo;s consider two examples:</p>
<ul>
<li><strong>ratio of 1 to 6</strong>: log(1/6) = log(0.17) = -1.79</li>
<li><strong>ratio of 6 to 1</strong>: log(6/1) = log(6) = 1.79</li>
</ul>
<p>So if the logs are against, the log(odds) will be negative up to infinity, while if they are in favor we see the opposite. By using the log function we ensure that the distance from the origin (0) is the same for 1:6 and 6:1, which wouldn&rsquo;t be possible otherwise.</p>
<p>The log-odds is the natural logarithm of the odds, expressed as:</p>
$$ \text{logit}(p) = \log\left(\frac{p}{1 - p}\right) $$
<h4 id="sigmoid-function">Sigmoid function</h4>
<p>To go from log-odds back to probability, we can use the sigmoid function:</p>
$$ \text{y'} = \frac{1}{1 + e^{-z}} $$
<p>Something worth keeping in mind is that the linear function from the logistic regression model becomes input to the sigmoid function, which turns it into a s-shape function. The log-odds are transformed from really large numbers of z into probabilities between 0 and 1, exclusive.</p>
<p><img alt="sigmoid" src="https://dinispeixoto.com/img/posts/logistic-regression/sigmoid.png"></p>
<p>So essentially the output of a logistic regression model is a log-odds, which can easily be transformed into a probability through the sigmoid function.</p>
<h3 id="loss-functions-log-loss">Loss Functions: Log loss</h3>
<p>The Log Loss equation returns the logarithm of the magnitude of the change, rather than just the distance from data to prediction.</p>
$$ \text{LogLoss}(y, \hat{y}) = - \left( y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right) $$
<p>It&rsquo;s worth understanding what would happen if we used the same loss function as a linear regression model, e.g. MSE. Let&rsquo;s assume we have 3 models - A, B and C, predicting 0.9, 0.6 and 0.0001, respectively for a true label example.</p>
<p><strong>Model A</strong>
</p>
$$ \text{MSE} = (0.9 - 1)^2 = 0.01 $$
$$ \text{LogLoss} = -\left(1 \cdot \log(0.9) + 0 \cdot \log(1 - 0.9)\right) = -\log(0.9) \approx 0.105 $$
<p><strong>Model B</strong>
</p>
$$ \text{MSE} = (0.6 - 1)^2 = 0.16 $$
$$ \text{LogLoss} = -\log(0.6) \approx -0.22 $$
<p><strong>Model C</strong>
</p>
$$ \text{MSE} = (0.0001 - 1)^2 \approx 0.99 $$
$$ \text{LogLoss} = -\log(0.0001) = -4 $$
<p>The difference in MSE for models A and B is just 0.15, which is incredibily small, even though model B is not doing a good job. MSE only sees “how close is the number to 1”, and 0.6 is numerically closer to 1 than, say, 0.2. It doesn’t understand that 0.6 means “not very confident”.</p>
<p>It&rsquo;s also possible to see in model C what happens when the model is incredibily wrong, for the possible label (y=1) the log loss becomes:</p>
$$ \text{LogLoss} = -\log(\hat{y}) $$
<p>So if the prediction is close to 0, the loss is going to be increasing towards \((+\infty\)). It&rsquo;s always worth checking how the log function works, for values from 1 and 0 in the x axis, y decreases rapidily, which is exactly what we are seeing here (although the inverse since the loss function is \(-\log(\text{x})\)). On the other hand, if the prediction is close to 1, the loss is going get closer to 0, as \(~log(1) = 0\).</p>
<p><img alt="log" src="https://dinispeixoto.com/img/posts/logistic-regression/log.png"></p>
<p>Similarly, when predicting the negative label, it becomes:</p>
$$ \text{LogLoss} = -\log(1 - \hat{y}) $$
<p>So the log loss is essentially \(-\log(\hat{y})\) if y = 1 or \(-\log(1 - \hat{y})\) if y = 0, which makes it quite straightforward to understand.</p>
<h3 id="recap">Recap</h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Equation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear model</td>
<td>\( z = w \cdot x + b \)</td>
<td>Weighted sum of inputs</td>
</tr>
<tr>
<td>Log-odds</td>
<td>\( \log\left( \frac{p}{1 - p} \right) = z \)</td>
<td>The linear model predicts log-odds. The log-odds scale is unbounded (\(-\infty\) to \(+\infty\)) perfect for linear models</td>
</tr>
<tr>
<td>Probability (sigmoid)</td>
<td>\( p = \frac{1}{1 + e^{-z}} \)</td>
<td>Converts log-odds to probability. Probability scale is bounded (0 to 1)</td>
</tr>
<tr>
<td>Log loss</td>
<td>\( -y \log(\hat{p}) - (1 - y) \log(1 - \hat{p}) \)</td>
<td>Penalizes confident wrong predictions</td>
</tr>
</tbody>
</table>
<h3 id="references">References</h3>
<ul>
<li><a href="https://developers.google.com/machine-learning/crash-course/logistic-regression">Logistic Regression</a></li>
<li><a href="https://www.youtube.com/watch?v=ARfXDSkQf1Y">Odds and Log(Odds)</a></li>
</ul>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://dinispeixoto.com/tags/machine-learning/">machine learning</a></span>
        
    </p>

      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="https://dinispeixoto.com/categories/machine-learning-engineering/">Machine Learning Engineering</a></span>
        
    </p>


      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        906 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2023-06-18 01:00 &#43;0100
        

         
          
        
      </p>
    </div>

    

    

    

  </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span></span>
            <span>&copy; 2025</span>
            
                <span><a href="https://dinispeixoto.com/">Dinis Peixoto</a></span>
            
            <span>Powered by <a href="http://gohugo.io">Hugo</a> & <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng">Hello Friend NG</a></span>
            <span></span>
        </div>
    </div>
</footer>



            
        </div>

        



<script type="text/javascript" src="https://dinispeixoto.com/bundle.min.b1dc444a7108174ae3d2294f982be5e99e844ae2b8d58eeea19d929bde0338be4d12ed8a4dfc67194dcc6726fa6a82705119c0d7e4edd4de9294be6535ea7b7f.js" integrity="sha512-sdxESnEIF0rj0ilPmCvl6Z6ESuK41Y7uoZ2Sm94DOL5NEu2KTfxnGU3MZyb6aoJwURnA1&#43;Tt1N6SlL5lNep7fw=="></script>



    </body>
</html>
