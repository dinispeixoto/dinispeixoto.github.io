<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Dinis Peixoto]">
<meta name="description" content="I started learning about Machine Learning with a couple online courses. The thing with these courses is that I got to learn a lot of cool stuff supported with step-by-step tutorials, but I never took some time to actually build something on my own, that tackles a real issue that I&amp;rsquo;m facing.
This is where things get interesting. One of the initiatives that I had at the company I was working back then was around brand recommendations." />
<meta name="keywords" content="dinis peixoto, machine learning, machine learning engineering, mlops, computer science, today I learned, blog, hugo, software engineering, resume, machine learning, brand recommendations, mle, tfidf, cosine similarity" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://dinispeixoto.com/posts/brand-recommendations-tf-idf/" />


    <title>
        
            Brand Recommendations - TF-IDF :: Dinis Peixoto 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://dinispeixoto.com/main.d2b9fd41d7e7e3c5253cd69761a2dfc0c0ffbff5d78e06960781a5ca3e8bf4ea.css">



    <link rel="apple-touch-icon" sizes="180x180" href="https://dinispeixoto.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dinispeixoto.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dinispeixoto.com/favicon-16x16.png">
    <link rel="manifest" href="https://dinispeixoto.com/site.webmanifest">
    <link rel="mask-icon" href="https://dinispeixoto.com/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="https://dinispeixoto.com/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Brand Recommendations - TF-IDF">
<meta itemprop="description" content="I started learning about Machine Learning with a couple online courses. The thing with these courses is that I got to learn a lot of cool stuff supported with step-by-step tutorials, but I never took some time to actually build something on my own, that tackles a real issue that I&rsquo;m facing.
This is where things get interesting. One of the initiatives that I had at the company I was working back then was around brand recommendations."><meta itemprop="datePublished" content="2020-03-13T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-03-13T00:00:00+00:00" />
<meta itemprop="wordCount" content="2647"><meta itemprop="image" content="https://dinispeixoto.com/img/dinis1.jpg" />
<meta itemprop="keywords" content="machine learning,brand recommendations,mle,tfidf,cosine similarity," />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://dinispeixoto.com/img/dinis1.jpg" /><meta name="twitter:title" content="Brand Recommendations - TF-IDF"/>
<meta name="twitter:description" content="I started learning about Machine Learning with a couple online courses. The thing with these courses is that I got to learn a lot of cool stuff supported with step-by-step tutorials, but I never took some time to actually build something on my own, that tackles a real issue that I&rsquo;m facing.
This is where things get interesting. One of the initiatives that I had at the company I was working back then was around brand recommendations."/>



    <meta property="og:title" content="Brand Recommendations - TF-IDF" />
<meta property="og:description" content="I started learning about Machine Learning with a couple online courses. The thing with these courses is that I got to learn a lot of cool stuff supported with step-by-step tutorials, but I never took some time to actually build something on my own, that tackles a real issue that I&rsquo;m facing.
This is where things get interesting. One of the initiatives that I had at the company I was working back then was around brand recommendations." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dinispeixoto.com/posts/brand-recommendations-tf-idf/" /><meta property="og:image" content="https://dinispeixoto.com/img/dinis1.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-03-13T00:00:00+00:00" />







    <meta property="article:section" content="Machine Learning Engineering" />



    <meta property="article:published_time" content="2020-03-13 00:00:00 &#43;0000 UTC" />









    
<script>
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-170600213-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://dinispeixoto.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text"> cd $HOME</span>
            <span class="logo__cursor" style=
                  "
                   
                   animation-duration:1s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dinispeixoto.com/about/">about me</a></li><li><a href="https://dinispeixoto.com/posts/">posts</a></li><li><a href="https://dinispeixoto.com/til/">today I learned</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        13 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://dinispeixoto.com/posts/brand-recommendations-tf-idf/">Brand Recommendations - TF-IDF</a>
      </h1>

      

      

      

      <div class="post-content">
        <p>I started learning about Machine Learning with <a href="https://www.udemy.com/course/machinelearning/">a</a> <a href="https://www.udemy.com/course/deeplearning/">couple</a> online courses. The thing with these courses is that I got to learn a lot of cool stuff supported with step-by-step tutorials, but I never took some time to actually build something on my own, that tackles a real issue that I&rsquo;m facing.</p>
<p>This is where things get interesting. One of the initiatives that I had at the company I was working back then was around brand recommendations. So I started wondering how hard could it be to build a simple machine learning model to provide brand recommendations.</p>
<p>This blog post will describe all the steps that were taken to build a baseline model that provides brand recommendations for the top brands available on well knwon e-commerce marketplace. Please note that the approach used here is not rocket science.</p>
<h2 id="how-does-a-recommendations-system-work">How does a recommendations system work?</h2>
<p>I will start by briefly explaining how the two most common types of recommendation systems work. Let&rsquo;s consider the example of a platform like Medium that has to recommend articles to users. It can either recommend articles to users based on their content, or based on what similar users tend to read.</p>
<ul>
<li>
<p><strong>Collaborative Filtering</strong>: providing article recommendations based on what other similar users also read. Based on the interactions that users have with the same articles, the system provides article recommendations with more confidence</p>
</li>
<li>
<p><strong>Content-Based Filtering</strong>: providing article recommendations based on the content of the article. If users usually read articles about <em>machine learning</em>, they will be more likely to read other articles that share the same topic</p>
</li>
</ul>
<p><img alt="Recommendation Systems" src="https://dinispeixoto.com/img/posts/brand-recommendations-tfidf/recommendation-systems.png"></p>
<p>Even though I find the Collaborative Filtering type of systems a lot more interesting, they are generally more challenging. I decided to start with a <strong>Content-Based Recommendations System</strong> as the only data that it will need are descriptions from products available on the website. Later on, I will consider building a Hybrid Recommendations System where both Content-Based and Collaborative Filtering can be used simultaneously.</p>
<p>The goal will be to build a Content-Based Brand Recommendations System that provides the Top N brand recommendations for a particular brand.</p>
<h2 id="term-frequency---inverse-document-frequency">Term Frequency - Inverse Document Frequency</h2>
<p>TF-IDF stands for <em>Term Frequency - Inverse Document Frequency</em> and is used to measure how important a keyword (or multiple keywords) is to a document in a collection of documents (e.g. blog posts on this website). TF-IDF is the product of two different frequencies:</p>
<ul>
<li>
<p><strong>Term Frequency</strong>: measures how frequent a term is in a document. It will increase if the number of times a term appears also increases.</p>
<p><code>TF(t) = Number of times term t appears in a document / Total number of terms in the document</code></p>
</li>
<li>
<p><strong>Inverse Document Frequency</strong>: measures how important a term is. It will increase if the number of documents that contain the term decreases, i.e. if a term is rare. Meaning that common English words (e.g. <em>the</em>, <em>or</em>, <em>a</em>) should be quite irrelevant as they will be common on all the documents (as long as those documents are written in English). On the other hand, if we consider all the articles on my blog, words like <em>brand</em>, <em>recommendations</em> and <em>machine-learning</em> will be particularly relevant for the article you are reading.</p>
<p><code>IDF(t) = log(Total number of documents / Number of documents with term t in it)</code></p>
<p>The logarithm is used to smooth the impact that higher values can have in the Inverse Document Frequency. Let&rsquo;s remove the <code>log</code> for a moment and consider a corpus that consists of one million documents where only one of them contains the term <em>machine-learning</em>. In this scenario we would have 1.000.000/1, thus an Inverse Document Frequency of 1.000.000, which in turn would make the Term Frequency completely useless.</p>
</li>
</ul>
<h3 id="tf-idf--tf-x-idf">TF-IDF = TF x IDF</h3>
<p>TF-IDF will give us the weight that each term has w.r.t. each particular document. We will consider each document to be a different brand. For each brand, we will build a brand description by merging some of their product descriptions. For the brand Adidas, we will bring together the descriptions of up to N products - stan smith, ultra boost, etc - and combine them into a brand description.</p>
<table>
<thead>
<tr>
<th></th>
<th>Term A</th>
<th>Term B</th>
<th>Term C</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brand A</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Brand B</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The result will be a <em>N_brands</em> x <em>N_terms</em> matrix, where each row is a vector that represents a given brand - <code>(weight_term_A, weight_term_B, weight_term_C)</code>. We won&rsquo;t be able to find which brands are related by only looking at this matrix. Instead, we need to find a way to build a matrix that compares each brand to every other brand - <em>N_brands</em> x <em>N_brands</em> - and this is where Cosine Similarities come in handy.</p>
<table>
<thead>
<tr>
<th></th>
<th>Brand A</th>
<th>Brand B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brand A</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Brand B</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="cosine-similarity">Cosine Similarity</h2>
<p>Well, we already have a vector for each of the brands. It&rsquo;s time to compare them to see which vectors are similar. Cosine Similarity is one of the most common strategies to compare how similar two vectors are. It is defined by the cosine of the angle between two vectors, and we will see later that it should be the same as the dot product of the two vectors when they are normalized, i.e. both have length 1.</p>
<p><img alt="Cosine Similarity" src="https://dinispeixoto.com/img/posts/brand-recommendations-tfidf/cosine_similarity.png"></p>
<p>Even though the cosine values range from -1 to 1, we will never have negative values in our vectors. The range will always be from 0 to 1, where values near 1 will mean that the vectors are similar, i.e. the brands that those vectors correspond to are similar and can be recommended together.</p>
<p>The easiest way to see this in practice is to consider two vectors that denote the same brand, which is the same as saying two equal vectors. If they are the same, then the angle between the two is 0. The cosine of its angle <code>cos(0)</code> is 1. When building our <em>N_brands</em> x <em>N_brands</em> matrix, we will see that, for each brand, the most similar brand (where the cosine similarity is 1) is the brand itself.</p>
<table>
<thead>
<tr>
<th></th>
<th>Brand A</th>
<th>Brand B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brand A</td>
<td>1</td>
<td>?</td>
</tr>
<tr>
<td>Brand B</td>
<td>?</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Let&rsquo;s now look at the cosine similarity formula. If our vectors are normalized, every vector has length 1, meaning that we can remove the fraction&rsquo;s denominator as it will also be 1. Without the denominator, the only thing left to calculate is the <em>dot product</em> of the vectors we want to compare.</p>
<p><img alt="Formula" src="https://dinispeixoto.com/img/posts/brand-recommendations-tfidf/formula.png"></p>
<p>If you look closely, the dot product should be straightforward, as each <code>Ai x Bi</code> will be the multiplication of the weights of the term <code>i</code> for the brands A and B. The image below should help understanding how the dot product of two vectors works.</p>
<p><img alt="Formula" src="https://dinispeixoto.com/img/posts/brand-recommendations-tfidf/dot_product.jpg"></p>
<p>So, for each brand, we will calculate the dot product that the brand&rsquo;s vector makes with every other brand&rsquo;s vector, which will result in the <em>N_brands</em> x <em>N_brands</em> matrix that we were seeking. To provide the Top N recommendations for a given brand, we will have to search for the brands that resulted in a higher dot product for the brand we are testing.</p>
<p>Looking at the example below, we can see that we would be more likely to recommend brand C (<code>0.8</code>) than brand B (<code>0.1</code>) to users that also like or buy products from brand A.</p>
<table>
<thead>
<tr>
<th></th>
<th>Brand A</th>
<th>Brand B</th>
<th>Brand C</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brand A</td>
<td>1</td>
<td>0.1</td>
<td>0.8</td>
</tr>
<tr>
<td>Brand B</td>
<td>0.1</td>
<td>1</td>
<td>0.3</td>
</tr>
<tr>
<td>Brand C</td>
<td>0.8</td>
<td>0.3</td>
<td>1</td>
</tr>
</tbody>
</table>
<h2 id="step-by-step">Step by step</h2>
<p>Now that we have gone through the theory of how both TF-IDF and Cosine Similarities work, we can start describing step-by-step how to build the recommendations model. All the code described below is available in a <a href="https://github.com/dinispeixoto/related-brands-mlops">Github repository</a>.</p>
<h3 id="gather-data">Gather Data</h3>
<p>The first step was to gather the data that the model will be trained with. For the sake of simplicity let&rsquo;s assume that we had access to the data in a Neo4j instance. The data (brands, products and their descriptions) is available on the website and could be easily retrieved with a scrapper and then loaded into Neo4j.</p>
<p>The e-commerce website has thousands of brands available. It&rsquo;s critical to filter a subset of brands that can be appropriate for the model. I have concluded that it would be worth filtering the brands with a significant number of products, e.g. at least 1000 products. Funny enough, the result is around 1000 brands as well.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> neo4j <span style="color:#f92672">import</span> GraphDatabase
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>db <span style="color:#f92672">=</span> GraphDatabase<span style="color:#f92672">.</span>driver(<span style="color:#e6db74">&#34;bolt://hostname:port&#34;</span>, auth<span style="color:#f92672">=</span>(username, password))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>MIN_PRODUCTS <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>statement <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">MATCH (brand:Brand)-[has_product:HAS_PRODUCT]-&gt;(product:Product)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">WHERE count(has_product) &gt; </span><span style="color:#e6db74">{</span>MIN_PRODUCTS<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">RETURN brand
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>brands <span style="color:#f92672">=</span> db<span style="color:#f92672">.</span>session()<span style="color:#f92672">.</span>run(statement)
</span></span><span style="display:flex;"><span>filtered_brand_ids <span style="color:#f92672">=</span> [brand[<span style="color:#e6db74">&#39;brandId&#39;</span>]) <span style="color:#66d9ef">for</span> brand <span style="color:#f92672">in</span> brands]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Total filtered brands: </span><span style="color:#e6db74">{</span>len(filtered_brand_ids)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>As soon as we have the brands that will be used by our model, it&rsquo;s time to gather the descriptions from up to 20000 of their products. A CSV file is created with the brand identifiers (id and name) and their corresponding description - i.e. the result of merging the descriptions from their products.</p>
<p>Keep in mind that concatenating all the product descriptions into a single brand description is naïve. In the future, we could reconsider and replace it with a better strategy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>statement <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">MATCH (brand:Brand { brandId: $id })-[has_product:HAS_PRODUCT]-&gt;(product:Product)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">WITH collect(product.description)[0..20000] as descriptions, brand
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">RETURN brand, descriptions
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># create a session to run all the neo4j queries</span>
</span></span><span style="display:flex;"><span>session <span style="color:#f92672">=</span> db<span style="color:#f92672">.</span>session()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>brand_descriptions <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>f <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#39;brand_descriptions.csv&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># csv file with: brand id, brand name, brand description</span>
</span></span><span style="display:flex;"><span>f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;brand_id,brand_name,description</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> brand_id <span style="color:#f92672">in</span> filtered_brand_ids:
</span></span><span style="display:flex;"><span>    brand_descriptions_neo4j <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>run(statement, id<span style="color:#f92672">=</span>brand_id)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> brand, descriptions <span style="color:#f92672">in</span> brand_descriptions_neo4j:
</span></span><span style="display:flex;"><span>        brand_name <span style="color:#f92672">=</span> brand[<span style="color:#e6db74">&#39;name&#39;</span>]
</span></span><span style="display:flex;"><span>        brand_descriptions[brand_id] <span style="color:#f92672">=</span> descriptions
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># concat product descriptions to build the brand description</span>
</span></span><span style="display:flex;"><span>    concated_descriptions <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(brand_descriptions[brand_id])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># pre-process descriptions</span>
</span></span><span style="display:flex;"><span>  	brand_description <span style="color:#f92672">=</span> pre_process(concated_descriptions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>brand_id<span style="color:#e6db74">}</span><span style="color:#e6db74">,</span><span style="color:#e6db74">{</span>brand_name<span style="color:#e6db74">}</span><span style="color:#e6db74">,</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">&#34;</span>{str(brand_description)}\\<span style="color:#e6db74">&#34; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#34;</span>)
</span></span><span style="display:flex;"><span>f<span style="color:#f92672">.</span>close()
</span></span></code></pre></div><h3 id="preparing-the-data">Preparing the data</h3>
<p>You might have noticed that each brand description is processed right before being written to the resulting CSV file.</p>
<p>There was nothing special when it comes to preparing the data. We are just replacing some special characters on the product descriptions to avoid having them on the final brand description, as they don&rsquo;t add any value. When training the model, we will also make sure to remove the most common English words (e.g. <em>a</em>, <em>or</em>, <em>the</em>) since they don&rsquo;t add any value either.</p>
<p>Finally, we convert all the text to lowercase since terms like <em>Red</em>, and <em>red</em> shouldn&rsquo;t be any different.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pre_process</span>(brand_description):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># pontuaction</span>
</span></span><span style="display:flex;"><span>    symbols <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;!</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">&#34;</span><span style="color:#75715e">#$%&amp;()&#39;*+-.,/:;&lt;=&gt;?@[\\]^_`{|}~\\n\\r&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> symbols:
</span></span><span style="display:flex;"><span>        brand_description <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>char<span style="color:#f92672">.</span>replace(brand_description, i, <span style="color:#e6db74">&#39; &#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># lowercase</span>
</span></span><span style="display:flex;"><span>    brand_description <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>char<span style="color:#f92672">.</span>lower(brand_description)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> str(brand_description)
</span></span></code></pre></div><h3 id="model-training">Model training</h3>
<p>The data is now collected and pre-processed. Everything should be ready to train the model. As we have already explained earlier, we will use TF-IDF to measure the weights for each term (or feature) within each document (i.e. brand description), and then Cosine Similarity to find similar documents.</p>
<p>First things first, let&rsquo;s read the content of the CSV file into a pandas data frame.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>brand_descriptions_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;brand_descriptions.csv&#34;</span>)
</span></span></code></pre></div><p>We then took advantage of the <code>TfidfVectorizer</code> class from <code>sklearn</code> to build a TF-IDF matrix. You can find all the possible parameters on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer&rsquo;s documentation</a>, but I will enumerate the few that were used:</p>
<ul>
<li><strong>analyzer</strong>: <code>word</code> - features (i.e. columns) will be made up of words or n-grams.</li>
<li><strong>ngram_range</strong>: <code>(1,4)</code> - features as n-grams from 1 to 4, e.g. <em>hello</em>, <em>hello world</em>, <em>hello world dinis</em>, <em>hello world dinis peixoto</em>.</li>
<li><strong>min_df</strong>: <code>0.05</code> - minimum document frequency, i.e. ignore terms that have a document frequency lower than the threshold (5%).</li>
<li><strong>stop_words</strong>: <code>english</code> - remove common english words from the description as they won&rsquo;t add value.</li>
</ul>
<p>We have also printed some of the features to show that they are actually quite relevant and descriptive of the products that a brand may have. They also include examples of different n-grams.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> TfidfVectorizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tfidf <span style="color:#f92672">=</span> TfidfVectorizer(
</span></span><span style="display:flex;"><span>  analyzer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;word&#39;</span>, ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>  min_df<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>, stop_words<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tfidf_matrix <span style="color:#f92672">=</span> tfidf<span style="color:#f92672">.</span>fit_transform(brand_descriptions_df[<span style="color:#e6db74">&#39;description&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># some examples: white wool, low sneakers, stainless, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># round glasses, multicolor synthetic, trousers</span>
</span></span><span style="display:flex;"><span>feature_names <span style="color:#f92672">=</span> tfidf<span style="color:#f92672">.</span>get_feature_names()
</span></span></code></pre></div><p>Then we compare each of the brand vectors, i.e. the rows in the TF-IDF matrix. As we have seen earlier, Cosine Similarity does not take into account the magnitude of the vectors. The <code>TfidfVectorizer</code> returns normalized weights (magnitude of 1), so the Linear Kernel is sufficient to calculate the similarity values that we are looking for.</p>
<p>We could be using the <code>cosine_similarity</code> from <code>sklearn</code> instead of <code>linear_kernel</code>. We would, however, be calculating the vector&rsquo;s magnitude without the need to, which can have a considerable impact on the performance when using a matrix with high dimensions. I&rsquo;d suggest further reading on this and the different pairwise metrics used to evaluate distances that <code>sklearn</code> provides - <a href="https://scikit-learn.org/stable/modules/metrics.html">Pairwise metrics, Affinities and Kernels</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics.pairwise <span style="color:#f92672">import</span> linear_kernel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cosine_similarities <span style="color:#f92672">=</span> linear_kernel(tfidf_matrix, tfidf_matrix)
</span></span></code></pre></div><p>Now that we have the matrix of Cosine Similarities, the only thing left to do is build a dictionary with the top N recommendations for each brand. Remember that the most similar brand is the brand itself? That&rsquo;s exactly what we are doing in the last line, excluding it from the final recommendations.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> {} <span style="color:#75715e"># (brand_id : [(score, similar_brand_id)]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> idx, row <span style="color:#f92672">in</span> brand_descriptions_df<span style="color:#f92672">.</span>iterrows(): 
</span></span><span style="display:flex;"><span>    similar_indices <span style="color:#f92672">=</span> cosine_similarities[idx]<span style="color:#f92672">.</span>argsort()[:<span style="color:#f92672">-</span><span style="color:#ae81ff">15</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    similar_items <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>      (cosine_similarities[idx][i], brand_descriptions_df[<span style="color:#e6db74">&#39;brand_id&#39;</span>][i]) 
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> similar_indices
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    results[row[<span style="color:#e6db74">&#39;brand_id&#39;</span>]] <span style="color:#f92672">=</span> similar_items[<span style="color:#ae81ff">1</span>:] <span style="color:#75715e"># excludes the brand itself</span>
</span></span></code></pre></div><h3 id="evaluation">Evaluation</h3>
<p>And we have finally reached the most fun part, actually providing brand recommendations. We start by creating a <code>recommend</code> function that gives the top N recommended brands (up to 14) based on a brand id received as input.</p>
<p>Note that model evaluation consists of evaluating a model&rsquo;s performance by comparing the results on the training set with the expected results from the test set (i.e. data not seen by the model). The thing is that I didn&rsquo;t have a curated list of brands that should be recommended together, so I just had a look at the results and tried to reach some conclusions around them. Evaluating the performance of recommendations is a really common issue and one of the main challenges of recommendation systems.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recommend</span>(id, num<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>): <span style="color:#75715e"># max is 15 - 1 = 14</span>
</span></span><span style="display:flex;"><span>    recs <span style="color:#f92672">=</span> results[id][:num]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> rec <span style="color:#f92672">in</span> recs:
</span></span><span style="display:flex;"><span>        score, brand_id <span style="color:#f92672">=</span> rec[<span style="color:#ae81ff">0</span>], rec[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        brand_name <span style="color:#f92672">=</span> brand_descriptions_df<span style="color:#f92672">.</span>loc[
</span></span><span style="display:flex;"><span>          brand_descriptions_df[<span style="color:#e6db74">&#39;brand_id&#39;</span>] <span style="color:#f92672">==</span> brand_id, <span style="color:#e6db74">&#39;brand_name&#39;</span>
</span></span><span style="display:flex;"><span>        ]<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Brand </span><span style="color:#e6db74">{</span>brand_name<span style="color:#e6db74">}</span><span style="color:#e6db74"> (</span><span style="color:#e6db74">{</span>brand_id<span style="color:#e6db74">}</span><span style="color:#e6db74">) with score </span><span style="color:#e6db74">{</span>str(score)<span style="color:#e6db74">}</span><span style="color:#e6db74"> &#34;</span>)
</span></span></code></pre></div><p>Without further ado, let&rsquo;s see the results and reach some conclusions around them.</p>
<p><img alt="Results" src="https://dinispeixoto.com/img/posts/brand-recommendations-tfidf/recs.png"></p>
<ol>
<li><strong>Nike</strong> and <strong>Jordan</strong> is really interesting since they belong to the same parent company. Also, Nike recommendations seem to be decent, to say the least.</li>
<li>Sister brands are recommended together, e.g. multiple <strong>Adidas collaborations</strong> and <strong>COMME DES GARÇONS</strong> labels.</li>
<li>Great to see that every recommendation for <strong>Tommy Junior</strong> is indeed a Junior/Kids brand.</li>
<li><strong>Gucci</strong> and other related (I mean <em>expensive</em>) brands also together - <strong>Versace</strong>, <strong>Givenchy</strong>, <strong>Balenciaga</strong>, <strong>Prada</strong> and <strong>Burberry</strong>.</li>
<li>I only own 2 sneaker brands: <strong>Adidas</strong> and <strong>Veja</strong>, so I&rsquo;m quite happy with the Adidas recommendations even without <strong>Nike</strong> being there.</li>
<li>Every recommendation for <strong>Rayban</strong> is also an eyewear brand.</li>
</ol>
<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>
<p>I guess this was only the beginning of my Machine Learning journey. What I built here is really simple but, in a way, tackles an issue that I found that needed to be tackled. It had a real-world use-case, which was what I was looking for. On top of that, even though the model is simple, the results ended up being quite interesting.</p>
<p>The next step is setting up a pipeline that trains and deploys the model so that I can explore a little bit more the MLOps world. In fact, I have already this in place (it&rsquo;s available in the same <a href="https://github.com/dinispeixoto/related-brands-mlops">Github repository</a>) and plan to write about the process soon.</p>
<p>My initial goal was to build a recommendations model based on collaborative filtering (it seems to be a lot more fun), so I might work on that in the future and combine both models into a <em>Hybrid Recommendations System</em>. Before doing so, I still have to figure out a way of evaluating the models to check whether the recommendations that it is making actually make any sense, other than my fashion judgement on which brands should be recommended together.</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://dinispeixoto.com/tags/machine-learning/">machine learning</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/brand-recommendations/">brand recommendations</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/mle/">mle</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/tfidf/">tfidf</a></span>
        <span class="tag"><a href="https://dinispeixoto.com/tags/cosine-similarity/">cosine similarity</a></span>
        
    </p>

      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="https://dinispeixoto.com/categories/machine-learning-engineering/">Machine Learning Engineering</a></span>
        
    </p>


      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        2647 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2020-03-13 00:00 &#43;0000
        

         
          
        
      </p>
    </div>

    

    

    

  </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span></span>
            <span>&copy; 2025</span>
            
                <span><a href="https://dinispeixoto.com/">Dinis Peixoto</a></span>
            
            <span>Powered by <a href="http://gohugo.io">Hugo</a> & <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng">Hello Friend NG</a></span>
            <span></span>
        </div>
    </div>
</footer>



            
        </div>

        



<script type="text/javascript" src="https://dinispeixoto.com/bundle.min.b1dc444a7108174ae3d2294f982be5e99e844ae2b8d58eeea19d929bde0338be4d12ed8a4dfc67194dcc6726fa6a82705119c0d7e4edd4de9294be6535ea7b7f.js" integrity="sha512-sdxESnEIF0rj0ilPmCvl6Z6ESuK41Y7uoZ2Sm94DOL5NEu2KTfxnGU3MZyb6aoJwURnA1&#43;Tt1N6SlL5lNep7fw=="></script>



    </body>
</html>
